{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da57d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c53cedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "\n",
    "class BertMovieModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertMovieModel, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "    def forward(self, input_embeddings, input_mask, token_type):\n",
    "        bert_output, pooled_output = self.bert(input_embeddings, attention_mask=input_mask, token_type_ids=token_type).to_tuple()\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        final_output = self.output(linear_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52593317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "\n",
    "lr = 0.0001\n",
    "max_len = 100\n",
    "num_epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f3092b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initializing the tokenizer for bert\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e146e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Dataset object for our data\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len, limit=-1):\n",
    "        tdf = pd.read_csv(path)\n",
    "        self.df = tdf.sample(frac=1)[:limit]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        token_type_ids = encoding['token_type_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.as_tensor(input_ids, dtype=torch.long),\n",
    "            'token_type_ids': torch.as_tensor(token_type_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.as_tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.as_tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08011a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our dataset instance and our dataloaders\n",
    "\n",
    "dataset = BertDataset('movie_reviews.csv', tokenizer, max_len)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b7c4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1:\n",
      "[CLS] this is a comedy based on national stereotypes, no doubt. if you leave away pretending you know or you care what communism was about and how real russians or brits are, if you accept and are not hurt by the conventions, you can have fun with this film. nicole kidman is at her best, sexy, moving and funny. ben chaplin succeeds to avoid being completely out - shadowed by nicole, and the rest of the cast does good work as well. the final is moving, [SEP]\n",
      "Label: Positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Review 2:\n",
      "[CLS] i read the book in 5th grade and now a few years later i saw the movie. there are a few differences : < br / > < br / > 1. billy was oringinally suppose to eat 15 worms in 15 days, not 10 worms in one day by 7 : 00pm. < br / > < br / > 2. billy is suppose to get 30 dollars after he's eaten all the worms. in the movie after billy eats all the worms, joe has [SEP]\n",
      "Label: Positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Review 3:\n",
      "[CLS] i came in in the middle of this film so i had no idea about any credits or even its title till i looked it up here, where i see that it has received a mixed reception by your commentators. i'm on the positive side regarding this film but one thing really caught my attention as i watched : the beautiful and sensitive score written in a coplandesque americana style. my surprise was great when i discovered the score to have been written by none other than john williams himself [SEP]\n",
      "Label: Positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Review 4:\n",
      "[CLS] as a kid i thought this movie was great. it had animals, it had beautiful music, and it had my favorite actor : michael j. fox. now, i still love this movie, for different reasons. it has well trained animals that are put through various stunts and scenes that look excellent on camera. it has beautiful, well - written musical that fits the scenes perfectly, with rousing fast - paced melodies and the heart wrenching main theme, that still makes me cry. even [SEP]\n",
      "Label: Positive\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Review 5:\n",
      "[CLS] this had high intellectual pretensions. the main lead intends to give a \" deep \" \" meaningful \" rendering ( with voice over for his frames of mind naturally ) and he was certainly influenced by the fifties / sixties \" method \" - which, when the script and the direction were worthwhile did give stunning results ( see clift, newman, winters ). but here the story is abysmal. besides it moves too slow, you could edit at least 20 minutes - including pointless [SEP]\n",
      "Label: Negative\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check some of the examples from our datasets\n",
    "\n",
    "def visualize_text_data(dataset, tokenizer, num_samples=5):\n",
    "    for i in range(min(len(dataset), num_samples)):\n",
    "        sample = dataset[i]\n",
    "        input_ids = sample['input_ids']\n",
    "        label = sample['labels'].item()\n",
    "\n",
    "        text = tokenizer.decode(input_ids)\n",
    "\n",
    "        print(f\"Review {i+1}:\")\n",
    "        print(text)\n",
    "        print(\"Label:\", \"Positive\" if label == 0 else \"Negative\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "visualize_text_data(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1432b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin epoch:  0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Training:  80%|████████  | 503/625 [1:10:46<08:41,  4.27s/it]"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "\n",
    "model = BertMovieModel()\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "optimizer= optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    print('Begin epoch: ', str(e) + '/' + str(num_epochs))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {e + 1}/{num_epochs} - Training\"):\n",
    "        ids = batch['input_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        label = batch['labels']\n",
    "        token_type = batch['token_type_ids']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(ids, mask, token_type).squeeze(1)\n",
    "        \n",
    "        loss = loss_fn(pred, label.float())\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Training Loss: {train_loss / len(train_dataloader)}\")\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            ids = batch['input_ids']\n",
    "            mask = batch['attention_mask']\n",
    "            label = batch['labels']\n",
    "            token_type = batch['token_type_ids']\n",
    "\n",
    "            pred = model(ids, mask, token_type).squeeze(1)\n",
    "\n",
    "            loss=loss_fn(pred, label.float())\n",
    "\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Validation Loss: {val_loss / len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69c34d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████| 79/79 [03:59<00:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5418167266906763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        ids = batch['input_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        token_type = batch['token_type_ids']\n",
    "\n",
    "        outputs = model(ids, mask, token_type)\n",
    "\n",
    "        predicted_labels = np.where(outputs >= 0.5, 1, 0)\n",
    "        \n",
    "        for prediction, label in zip(predicted_labels.tolist(), labels.tolist()):\n",
    "            if prediction[0] == label:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "            \n",
    "print('Accuracy:', correct_predictions/total_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
